import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
from datetime import datetime, timedelta
import json


class ClimateAnalytics:
    def __init__(self):
        self.scaler = StandardScaler()

    def prepare_data(self, sensor_data):
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"""
        if not sensor_data:
            return None

        df = pd.DataFrame(sensor_data)
        df['datetime'] = pd.to_datetime(df['timestamp'], unit='s')
        df = df.sort_values('datetime')

        # –î–æ–±–∞–≤–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        df['hour'] = df['datetime'].dt.hour
        df['day_of_week'] = df['datetime'].dt.dayofweek
        df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)

        return df

    def predict_trends(self, sensor_data, hours_ahead=6):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ç–µ–Ω–¥–µ–Ω—Ü–∏–π –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–µ N —á–∞—Å–æ–≤"""
        try:
            df = self.prepare_data(sensor_data)
            if df is None or len(df) < 10:
                return {"error": "–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞"}

            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã
            X = np.array(range(len(df))).reshape(-1, 1)
            y_temp = df['temp'].values

            # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å –ª–∏–Ω–µ–π–Ω–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏
            model_temp = LinearRegression()
            model_temp.fit(X, y_temp)

            # –ü—Ä–æ–≥–Ω–æ–∑ –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–µ hours_ahead –ø–µ—Ä–∏–æ–¥–æ–≤
            future_X = np.array(range(len(df), len(df) + hours_ahead)).reshape(-1, 1)
            future_temp = model_temp.predict(future_X)

            # –ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–∞
            current_trend = "—Å—Ç–∞–±–∏–ª—å–Ω—ã–π"
            if len(future_temp) > 1:
                trend_slope = future_temp[-1] - future_temp[0]
                if abs(trend_slope) < 0.5:
                    current_trend = "—Å—Ç–∞–±–∏–ª—å–Ω—ã–π"
                elif trend_slope > 0:
                    current_trend = "—Ä–æ—Å—Ç"
                else:
                    current_trend = "–ø–∞–¥–µ–Ω–∏–µ"

            return {
                "trend": current_trend,
                "predicted_temp": round(float(future_temp[-1]), 1),
                "confidence": "–≤—ã—Å–æ–∫–∞—è" if len(df) > 50 else "—Å—Ä–µ–¥–Ω—è—è",
                "next_hours": [
                    {
                        "hour": i + 1,
                        "temp": round(float(temp), 1)
                    }
                    for i, temp in enumerate(future_temp)
                ]
            }

        except Exception as e:
            return {"error": f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è: {str(e)}"}

    def analyze_correlations(self, sensor_data):
        """–ê–Ω–∞–ª–∏–∑ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π –º–µ–∂–¥—É –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏"""
        try:
            df = self.prepare_data(sensor_data)
            if df is None or len(df) < 10:
                return {"error": "–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"}

            # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
            correlations = df[['temp', 'hum', 'lux']].corr()

            temp_hum_corr = correlations.loc['temp', 'hum']
            temp_lux_corr = correlations.loc['temp', 'lux']
            hum_lux_corr = correlations.loc['hum', 'lux']

            # –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π
            def interpret_correlation(corr):
                abs_corr = abs(corr)
                if abs_corr < 0.3:
                    return "—Å–ª–∞–±–∞—è", "–Ω–µ—Ç –∑–Ω–∞—á–∏–º–æ–π —Å–≤—è–∑–∏"
                elif abs_corr < 0.7:
                    return "—É–º–µ—Ä–µ–Ω–Ω–∞—è", "–∑–∞–º–µ—Ç–Ω–∞—è –≤–∑–∞–∏–º–æ—Å–≤—è–∑—å"
                else:
                    return "—Å–∏–ª—å–Ω–∞—è", "—Ç–µ—Å–Ω–∞—è –≤–∑–∞–∏–º–æ—Å–≤—è–∑—å"

            temp_hum_strength, temp_hum_meaning = interpret_correlation(temp_hum_corr)
            temp_lux_strength, temp_lux_meaning = interpret_correlation(temp_lux_corr)
            hum_lux_strength, hum_lux_meaning = interpret_correlation(hum_lux_corr)

            return {
                "correlations": {
                    "temp_hum": {
                        "value": round(temp_hum_corr, 3),
                        "strength": temp_hum_strength,
                        "meaning": temp_hum_meaning,
                        "interpretation": "—Ä–æ—Å—Ç —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã ‚Üí —Å–Ω–∏–∂–µ–Ω–∏–µ –≤–ª–∞–∂–Ω–æ—Å—Ç–∏" if temp_hum_corr < 0 else "—Ä–æ—Å—Ç —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã ‚Üí —Ä–æ—Å—Ç –≤–ª–∞–∂–Ω–æ—Å—Ç–∏"
                    },
                    "temp_lux": {
                        "value": round(temp_lux_corr, 3),
                        "strength": temp_lux_strength,
                        "meaning": temp_lux_meaning,
                        "interpretation": "–æ—Å–≤–µ—â–µ–Ω–Ω–æ—Å—Ç—å –≤–ª–∏—è–µ—Ç –Ω–∞ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É" if abs(
                            temp_lux_corr) > 0.3 else "—Å–≤—è–∑—å –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∞"
                    },
                    "hum_lux": {
                        "value": round(hum_lux_corr, 3),
                        "strength": hum_lux_strength,
                        "meaning": hum_lux_meaning
                    }
                },
                "insights": self.generate_insights(df, temp_hum_corr, temp_lux_corr)
            }

        except Exception as e:
            return {"error": f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π: {str(e)}"}

    def generate_insights(self, df, temp_hum_corr, temp_lux_corr):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–Ω—Å–∞–π—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞–Ω–Ω—ã—Ö"""
        insights = []

        # –ê–Ω–∞–ª–∏–∑ —Å—É—Ç–æ—á–Ω—ã—Ö –∫–æ–ª–µ–±–∞–Ω–∏–π
        daily_avg = df.groupby('hour').agg({
            'temp': ['mean', 'std'],
            'hum': ['mean', 'std']
        }).round(2)

        max_temp_hour = daily_avg[('temp', 'mean')].idxmax()
        min_temp_hour = daily_avg[('temp', 'mean')].idxmin()

        insights.append(f"üìà –ü–∏–∫ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã –æ–±—ã—á–Ω–æ –≤ {max_temp_hour}:00")
        insights.append(f"üìâ –ú–∏–Ω–∏–º—É–º —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã –æ–±—ã—á–Ω–æ –≤ {min_temp_hour}:00")

        # –ê–Ω–∞–ª–∏–∑ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        temp_std = df['temp'].std()
        if temp_std < 1.0:
            insights.append("üå°Ô∏è –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –æ—á–µ–Ω—å —Å—Ç–∞–±–∏–ª—å–Ω–∞")
        elif temp_std > 3.0:
            insights.append("üå°Ô∏è –ó–∞–º–µ—Ç–Ω—ã –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–µ–±–∞–Ω–∏—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã")

        # –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—ã–µ –∏–Ω—Å–∞–π—Ç—ã
        if temp_hum_corr < -0.5:
            insights.append("üîÅ –°–∏–ª—å–Ω–∞—è –æ–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å: —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ ‚Üë ‚Üí –≤–ª–∞–∂–Ω–æ—Å—Ç—å ‚Üì")
        elif temp_lux_corr > 0.5:
            insights.append("üí° –û—Å–≤–µ—â–µ–Ω–Ω–æ—Å—Ç—å –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –≤–ª–∏—è–µ—Ç –Ω–∞ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É")

        return insights

    def detect_anomalies(self, sensor_data):
        """–û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π –≤ –¥–∞–Ω–Ω—ã—Ö"""
        try:
            df = self.prepare_data(sensor_data)
            if df is None or len(df) < 20:
                return {"error": "–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∞–Ω–æ–º–∞–ª–∏–π"}

            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∞–Ω–æ–º–∞–ª–∏–π
            features = df[['temp', 'hum', 'lux']].copy()

            # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
            scaled_features = self.scaler.fit_transform(features)

            # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∞–Ω–æ–º–∞–ª–∏–π
            iso_forest = IsolationForest(contamination=0.1, random_state=42)
            anomalies = iso_forest.fit_predict(scaled_features)

            # –ü–æ–º–µ—á–∞–µ–º –∞–Ω–æ–º–∞–ª–∏–∏ (-1 - –∞–Ω–æ–º–∞–ª–∏—è, 1 - –Ω–æ—Ä–º–∞)
            df['is_anomaly'] = anomalies
            df['anomaly_score'] = iso_forest.decision_function(scaled_features)

            # –ü–æ–ª—É—á–∞–µ–º –∞–Ω–æ–º–∞–ª—å–Ω—ã–µ —Ç–æ—á–∫–∏
            anomaly_points = df[df['is_anomaly'] == -1]

            # –ê–Ω–∞–ª–∏–∑ —Ç–∏–ø–æ–≤ –∞–Ω–æ–º–∞–ª–∏–π
            anomaly_analysis = []
            for _, row in anomaly_points.iterrows():
                anomaly_type = self.classify_anomaly(row)
                anomaly_analysis.append({
                    "timestamp": int(row['timestamp']),
                    "datetime": row['datetime'].strftime('%Y-%m-%d %H:%M'),
                    "temp": round(row['temp'], 1),
                    "hum": round(row['hum'], 1),
                    "lux": int(row['lux']),
                    "type": anomaly_type,
                    "score": round(row['anomaly_score'], 3)
                })

            return {
                "total_anomalies": len(anomaly_points),
                "anomaly_rate": round(len(anomaly_points) / len(df) * 100, 1),
                "anomalies": anomaly_analysis,
                "summary": self.anomaly_summary(anomaly_analysis)
            }

        except Exception as e:
            return {"error": f"–û—à–∏–±–∫–∞ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∞–Ω–æ–º–∞–ª–∏–π: {str(e)}"}

    def classify_anomaly(self, row):
        """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–∏–ø–∞ –∞–Ω–æ–º–∞–ª–∏–∏"""
        conditions = []

        if row['temp'] > 28:
            conditions.append("–≤—ã—Å–æ–∫–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞")
        elif row['temp'] < 15:
            conditions.append("–Ω–∏–∑–∫–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞")

        if row['hum'] > 75:
            conditions.append("–≤—ã—Å–æ–∫–∞—è –≤–ª–∞–∂–Ω–æ—Å—Ç—å")
        elif row['hum'] < 30:
            conditions.append("–Ω–∏–∑–∫–∞—è –≤–ª–∞–∂–Ω–æ—Å—Ç—å")

        if row['lux'] > 1500:
            conditions.append("—è—Ä–∫–æ–µ –æ—Å–≤–µ—â–µ–Ω–∏–µ")
        elif row['lux'] < 50:
            conditions.append("—Ç–µ–º–Ω–æ—Ç–∞")

        return ", ".join(conditions) if conditions else "–Ω–µ–æ–±—ã—á–Ω–æ–µ —Å–æ—á–µ—Ç–∞–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"

    def anomaly_summary(self, anomalies):
        """–°–≤–æ–¥–∫–∞ –ø–æ –∞–Ω–æ–º–∞–ª–∏—è–º"""
        if not anomalies:
            return "–ê–Ω–æ–º–∞–ª–∏–π –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ"

        types = {}
        for anomaly in anomalies:
            for anomaly_type in anomaly['type'].split(', '):
                types[anomaly_type] = types.get(anomaly_type, 0) + 1

        summary_parts = []
        for anomaly_type, count in types.items():
            summary_parts.append(f"{count}√ó {anomaly_type}")

        return "; ".join(summary_parts)


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –∞–Ω–∞–ª–∏—Ç–∏–∫–∏
analytics_engine = ClimateAnalytics()